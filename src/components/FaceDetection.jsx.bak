import React, { useRef, useEffect, useState } from 'react';
import * as faceapi from 'face-api.js';

const FaceDetection = () => {
    // Reference                }
            } catch (error) {
                console.error('Error in face detection:', error);
            }

            // Continue the detection loop
            requestRef.current = requestAnimationFrame(detectFace);
        };

        detectFace();

        return () => {
            if (requestRef.current) {
                cancelAnimationFrame(requestRef.current);
            }
        };
    }, [isVideoPlaying]);f = useRef(null);
    const canvasRef = useRef(null);
    const requestRef = useRef(null);

    // State
    const [isModelLoaded, setIsModelLoaded] = useState(false);
    const [isVideoPlaying, setIsVideoPlaying] = useState(false);
    const [error, setError] = useState(null);

    // Load models
    useEffect(() => {
        const loadModels = async () => {
            try {
                await Promise.all([
                    faceapi.nets.tinyFaceDetector.loadFromUri('/models'),
                    faceapi.nets.faceExpressionNet.loadFromUri('/models'),
                    faceapi.nets.faceLandmark68Net.loadFromUri('/models')
                ]);
                console.log('Face detection models loaded successfully');
                setIsModelLoaded(true);
            } catch (error) {
                console.error('Error loading models:', error);
                setError('Failed to load face detection models');
            }
        };
        loadModels();
    }, []);

    // Initialize webcam
    useEffect(() => {
        if (!isModelLoaded) return;

        const startVideo = async () => {
            try {
                const stream = await navigator.mediaDevices.getUserMedia({
                    video: {
                        width: 640,
                        height: 480,
                        facingMode: 'user',
                        frameRate: 30
                    }
                });

                if (videoRef.current) {
                    videoRef.current.srcObject = stream;
                    videoRef.current.onloadedmetadata = () => {
                        videoRef.current.play();
                        setIsVideoPlaying(true);
                    };
                }
            } catch (error) {
                console.error('Error accessing webcam:', error);
                let message = 'Failed to access webcam';
                if (error.name === 'NotAllowedError') {
                    message = 'Please allow camera access to use this feature';
                } else if (error.name === 'NotFoundError') {
                    message = 'No camera found';
                }
                setError(message);
            }
        };

        startVideo();

        return () => {
            if (videoRef.current?.srcObject) {
                videoRef.current.srcObject.getTracks().forEach(track => track.stop());
            }
        };
    }, [isModelLoaded]);

    // Face detection loop
    useEffect(() => {
        if (!isVideoPlaying || !videoRef.current || !canvasRef.current) return;

        const detectFace = async () => {
            try {
                const video = videoRef.current;
                const canvas = canvasRef.current;

                // Set canvas size to match video
                const displaySize = { width: video.videoWidth, height: video.videoHeight };
                faceapi.matchDimensions(canvas, displaySize);

                // Detect faces and expressions
                const detections = await faceapi
                    .detectAllFaces(video, new faceapi.TinyFaceDetectorOptions({ inputSize: 512 }))
                    .withFaceLandmarks()
                    .withFaceExpressions();

                // Clear previous drawings
                const ctx = canvas.getContext('2d');
                ctx.clearRect(0, 0, canvas.width, canvas.height);

                // Draw results
                if (detections.length === 0) {
                    // No face detected message
                    ctx.fillStyle = 'rgba(0, 0, 0, 0.7)';
                    ctx.fillRect(10, 10, 200, 40);
                    ctx.strokeStyle = '#FF4444';
                    ctx.strokeRect(10, 10, 200, 40);
                    ctx.fillStyle = '#FFFFFF';
                    ctx.font = 'bold 20px Arial';
                    ctx.fillText('No face detected', 20, 35);
                } else {
                    // Draw detections
                    const resizedDetections = faceapi.resizeResults(detections, displaySize);
                    resizedDetections.forEach(detection => {
                        // Draw face box
                        const box = detection.detection.box;
                        ctx.lineWidth = 3;
                        ctx.strokeStyle = '#00FF00';
                        ctx.strokeRect(box.x, box.y, box.width, box.height);

                        // Get and sort emotions
                        const emotions = detection.expressions;
                        const sortedEmotions = Object.entries(emotions)
                            .sort(([, a], [, b]) => b - a)
                            .slice(0, 2);

                        // Draw emotions with background
                        ctx.font = 'bold 16px Arial';
                        sortedEmotions.forEach(([emotion, confidence], index) => {
                            const text = `${emotion}: ${(confidence * 100).toFixed(0)}%`;
                            const y = box.bottom + 25 + (index * 25);
                            
                            // Background
                            const textWidth = ctx.measureText(text).width;
                            ctx.fillStyle = 'rgba(0, 0, 0, 0.7)';
                            ctx.fillRect(box.x, y - 20, textWidth + 10, 25);
                            
                            // Text
                            ctx.fillStyle = '#00FF00';
                            ctx.fillText(text, box.x + 5, y);
                        });

                        // Draw face landmarks
                        ctx.lineWidth = 1;
                        ctx.strokeStyle = '#00FF00';
                        ctx.fillStyle = '#00FF00';
                        detection.landmarks.positions.forEach(point => {
                            ctx.beginPath();
                            ctx.arc(point.x, point.y, 2, 0, 2 * Math.PI);
                            ctx.fill();
                        });
                    });
                }
            } catch (error) {
                console.error('Error in face detection:', error);
            }

            // Continue the detection loop
            animationFrameId = requestAnimationFrame(detectFace);
        };

        if (isVideoPlaying) {
            detectFace();
        }

        // Cleanup
        return () => {
            if (animationFrameId) {
                cancelAnimationFrame(animationFrameId);
            }
        };
    }, [isVideoPlaying]);

    // Render component
    return (
        <div className="relative w-full max-w-2xl mx-auto mt-4">
            {/* Error message */}
            {error && (
                <div className="absolute inset-x-0 top-0 z-30 p-4 bg-red-500 text-white text-center rounded-t-lg">
                    {error}
                </div>
            )}

            {/* Loading overlay */}
            {!isModelLoaded && (
                <div className="absolute inset-0 z-20 flex items-center justify-center bg-black bg-opacity-75 rounded-lg">
                    <div className="text-center text-white">
                        <div className="w-8 h-8 mx-auto mb-2 border-t-2 border-r-2 border-white rounded-full animate-spin" />
                        <p>Loading face detection models...</p>
                    </div>
                </div>
            )}
            
            {/* Video container */}
            <div className="relative aspect-video bg-gray-900 rounded-lg overflow-hidden">
                <video
                    ref={videoRef}
                    className="absolute inset-0 w-full h-full object-cover"
                    style={{ transform: 'scaleX(-1)' }}
                    autoPlay
                    playsInline
                    muted
                />
                <canvas
                    ref={canvasRef}
                    className="absolute inset-0 w-full h-full"
                    style={{ transform: 'scaleX(-1)' }}
                />
            </div>
        </div>
    );
};

export default FaceDetection;